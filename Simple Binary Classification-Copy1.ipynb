{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d845249e",
   "metadata": {},
   "source": [
    "# Tarea 2: Simple Binary Classification\n",
    "**Matematicas para el Aprendizaje de Máquina**\n",
    "\n",
    "**Universidad Nacional de Colombia**\n",
    "\n",
    "**2022**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf0316",
   "metadata": {},
   "source": [
    "El Sonar Dataset contiene la predicción de si un objeto es una roca (etiquetada con \"R\"), o una mina (etiquetada con \"M\"); dada la fuerza de los retornos del Sonar en diferentes angulos. \n",
    "\n",
    "A continuacion, a partir del DataSet, vamos a usar conocimientos de Machine Learning para estudiar esta clasificacion Binaria, y veremos cual método es el que mejor predice, tomando una métrica particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157dbb05",
   "metadata": {},
   "source": [
    "Primero, importamos el conjunto de datos a nuestro Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac1422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>Column2</th><th>Column3</th><th>Column4</th><th>Column5</th><th>Column6</th><th>Column7</th><th>Column8</th><th>Column9</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>208 rows × 61 columns (omitted printing of 52 columns)</p><tr><th>1</th><td>0.02</td><td>0.0371</td><td>0.0428</td><td>0.0207</td><td>0.0954</td><td>0.0986</td><td>0.1539</td><td>0.1601</td><td>0.3109</td></tr><tr><th>2</th><td>0.0453</td><td>0.0523</td><td>0.0843</td><td>0.0689</td><td>0.1183</td><td>0.2583</td><td>0.2156</td><td>0.3481</td><td>0.3337</td></tr><tr><th>3</th><td>0.0262</td><td>0.0582</td><td>0.1099</td><td>0.1083</td><td>0.0974</td><td>0.228</td><td>0.2431</td><td>0.3771</td><td>0.5598</td></tr><tr><th>4</th><td>0.01</td><td>0.0171</td><td>0.0623</td><td>0.0205</td><td>0.0205</td><td>0.0368</td><td>0.1098</td><td>0.1276</td><td>0.0598</td></tr><tr><th>5</th><td>0.0762</td><td>0.0666</td><td>0.0481</td><td>0.0394</td><td>0.059</td><td>0.0649</td><td>0.1209</td><td>0.2467</td><td>0.3564</td></tr><tr><th>6</th><td>0.0286</td><td>0.0453</td><td>0.0277</td><td>0.0174</td><td>0.0384</td><td>0.099</td><td>0.1201</td><td>0.1833</td><td>0.2105</td></tr><tr><th>7</th><td>0.0317</td><td>0.0956</td><td>0.1321</td><td>0.1408</td><td>0.1674</td><td>0.171</td><td>0.0731</td><td>0.1401</td><td>0.2083</td></tr><tr><th>8</th><td>0.0519</td><td>0.0548</td><td>0.0842</td><td>0.0319</td><td>0.1158</td><td>0.0922</td><td>0.1027</td><td>0.0613</td><td>0.1465</td></tr><tr><th>9</th><td>0.0223</td><td>0.0375</td><td>0.0484</td><td>0.0475</td><td>0.0647</td><td>0.0591</td><td>0.0753</td><td>0.0098</td><td>0.0684</td></tr><tr><th>10</th><td>0.0164</td><td>0.0173</td><td>0.0347</td><td>0.007</td><td>0.0187</td><td>0.0671</td><td>0.1056</td><td>0.0697</td><td>0.0962</td></tr><tr><th>11</th><td>0.0039</td><td>0.0063</td><td>0.0152</td><td>0.0336</td><td>0.031</td><td>0.0284</td><td>0.0396</td><td>0.0272</td><td>0.0323</td></tr><tr><th>12</th><td>0.0123</td><td>0.0309</td><td>0.0169</td><td>0.0313</td><td>0.0358</td><td>0.0102</td><td>0.0182</td><td>0.0579</td><td>0.1122</td></tr><tr><th>13</th><td>0.0079</td><td>0.0086</td><td>0.0055</td><td>0.025</td><td>0.0344</td><td>0.0546</td><td>0.0528</td><td>0.0958</td><td>0.1009</td></tr><tr><th>14</th><td>0.009</td><td>0.0062</td><td>0.0253</td><td>0.0489</td><td>0.1197</td><td>0.1589</td><td>0.1392</td><td>0.0987</td><td>0.0955</td></tr><tr><th>15</th><td>0.0124</td><td>0.0433</td><td>0.0604</td><td>0.0449</td><td>0.0597</td><td>0.0355</td><td>0.0531</td><td>0.0343</td><td>0.1052</td></tr><tr><th>16</th><td>0.0298</td><td>0.0615</td><td>0.065</td><td>0.0921</td><td>0.1615</td><td>0.2294</td><td>0.2176</td><td>0.2033</td><td>0.1459</td></tr><tr><th>17</th><td>0.0352</td><td>0.0116</td><td>0.0191</td><td>0.0469</td><td>0.0737</td><td>0.1185</td><td>0.1683</td><td>0.1541</td><td>0.1466</td></tr><tr><th>18</th><td>0.0192</td><td>0.0607</td><td>0.0378</td><td>0.0774</td><td>0.1388</td><td>0.0809</td><td>0.0568</td><td>0.0219</td><td>0.1037</td></tr><tr><th>19</th><td>0.027</td><td>0.0092</td><td>0.0145</td><td>0.0278</td><td>0.0412</td><td>0.0757</td><td>0.1026</td><td>0.1138</td><td>0.0794</td></tr><tr><th>20</th><td>0.0126</td><td>0.0149</td><td>0.0641</td><td>0.1732</td><td>0.2565</td><td>0.2559</td><td>0.2947</td><td>0.411</td><td>0.4983</td></tr><tr><th>21</th><td>0.0473</td><td>0.0509</td><td>0.0819</td><td>0.1252</td><td>0.1783</td><td>0.307</td><td>0.3008</td><td>0.2362</td><td>0.383</td></tr><tr><th>22</th><td>0.0664</td><td>0.0575</td><td>0.0842</td><td>0.0372</td><td>0.0458</td><td>0.0771</td><td>0.0771</td><td>0.113</td><td>0.2353</td></tr><tr><th>23</th><td>0.0099</td><td>0.0484</td><td>0.0299</td><td>0.0297</td><td>0.0652</td><td>0.1077</td><td>0.2363</td><td>0.2385</td><td>0.0075</td></tr><tr><th>24</th><td>0.0115</td><td>0.015</td><td>0.0136</td><td>0.0076</td><td>0.0211</td><td>0.1058</td><td>0.1023</td><td>0.044</td><td>0.0931</td></tr><tr><th>25</th><td>0.0293</td><td>0.0644</td><td>0.039</td><td>0.0173</td><td>0.0476</td><td>0.0816</td><td>0.0993</td><td>0.0315</td><td>0.0736</td></tr><tr><th>26</th><td>0.0201</td><td>0.0026</td><td>0.0138</td><td>0.0062</td><td>0.0133</td><td>0.0151</td><td>0.0541</td><td>0.021</td><td>0.0505</td></tr><tr><th>27</th><td>0.0151</td><td>0.032</td><td>0.0599</td><td>0.105</td><td>0.1163</td><td>0.1734</td><td>0.1679</td><td>0.1119</td><td>0.0889</td></tr><tr><th>28</th><td>0.0177</td><td>0.03</td><td>0.0288</td><td>0.0394</td><td>0.063</td><td>0.0526</td><td>0.0688</td><td>0.0633</td><td>0.0624</td></tr><tr><th>29</th><td>0.01</td><td>0.0275</td><td>0.019</td><td>0.0371</td><td>0.0416</td><td>0.0201</td><td>0.0314</td><td>0.0651</td><td>0.1896</td></tr><tr><th>30</th><td>0.0189</td><td>0.0308</td><td>0.0197</td><td>0.0622</td><td>0.008</td><td>0.0789</td><td>0.144</td><td>0.1451</td><td>0.1789</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Column1 & Column2 & Column3 & Column4 & Column5 & Column6 & Column7 & Column8 & Column9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.02 & 0.0371 & 0.0428 & 0.0207 & 0.0954 & 0.0986 & 0.1539 & 0.1601 & 0.3109 & $\\dots$ \\\\\n",
       "\t2 & 0.0453 & 0.0523 & 0.0843 & 0.0689 & 0.1183 & 0.2583 & 0.2156 & 0.3481 & 0.3337 & $\\dots$ \\\\\n",
       "\t3 & 0.0262 & 0.0582 & 0.1099 & 0.1083 & 0.0974 & 0.228 & 0.2431 & 0.3771 & 0.5598 & $\\dots$ \\\\\n",
       "\t4 & 0.01 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & 0.0368 & 0.1098 & 0.1276 & 0.0598 & $\\dots$ \\\\\n",
       "\t5 & 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.059 & 0.0649 & 0.1209 & 0.2467 & 0.3564 & $\\dots$ \\\\\n",
       "\t6 & 0.0286 & 0.0453 & 0.0277 & 0.0174 & 0.0384 & 0.099 & 0.1201 & 0.1833 & 0.2105 & $\\dots$ \\\\\n",
       "\t7 & 0.0317 & 0.0956 & 0.1321 & 0.1408 & 0.1674 & 0.171 & 0.0731 & 0.1401 & 0.2083 & $\\dots$ \\\\\n",
       "\t8 & 0.0519 & 0.0548 & 0.0842 & 0.0319 & 0.1158 & 0.0922 & 0.1027 & 0.0613 & 0.1465 & $\\dots$ \\\\\n",
       "\t9 & 0.0223 & 0.0375 & 0.0484 & 0.0475 & 0.0647 & 0.0591 & 0.0753 & 0.0098 & 0.0684 & $\\dots$ \\\\\n",
       "\t10 & 0.0164 & 0.0173 & 0.0347 & 0.007 & 0.0187 & 0.0671 & 0.1056 & 0.0697 & 0.0962 & $\\dots$ \\\\\n",
       "\t11 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & 0.0323 & $\\dots$ \\\\\n",
       "\t12 & 0.0123 & 0.0309 & 0.0169 & 0.0313 & 0.0358 & 0.0102 & 0.0182 & 0.0579 & 0.1122 & $\\dots$ \\\\\n",
       "\t13 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & 0.1009 & $\\dots$ \\\\\n",
       "\t14 & 0.009 & 0.0062 & 0.0253 & 0.0489 & 0.1197 & 0.1589 & 0.1392 & 0.0987 & 0.0955 & $\\dots$ \\\\\n",
       "\t15 & 0.0124 & 0.0433 & 0.0604 & 0.0449 & 0.0597 & 0.0355 & 0.0531 & 0.0343 & 0.1052 & $\\dots$ \\\\\n",
       "\t16 & 0.0298 & 0.0615 & 0.065 & 0.0921 & 0.1615 & 0.2294 & 0.2176 & 0.2033 & 0.1459 & $\\dots$ \\\\\n",
       "\t17 & 0.0352 & 0.0116 & 0.0191 & 0.0469 & 0.0737 & 0.1185 & 0.1683 & 0.1541 & 0.1466 & $\\dots$ \\\\\n",
       "\t18 & 0.0192 & 0.0607 & 0.0378 & 0.0774 & 0.1388 & 0.0809 & 0.0568 & 0.0219 & 0.1037 & $\\dots$ \\\\\n",
       "\t19 & 0.027 & 0.0092 & 0.0145 & 0.0278 & 0.0412 & 0.0757 & 0.1026 & 0.1138 & 0.0794 & $\\dots$ \\\\\n",
       "\t20 & 0.0126 & 0.0149 & 0.0641 & 0.1732 & 0.2565 & 0.2559 & 0.2947 & 0.411 & 0.4983 & $\\dots$ \\\\\n",
       "\t21 & 0.0473 & 0.0509 & 0.0819 & 0.1252 & 0.1783 & 0.307 & 0.3008 & 0.2362 & 0.383 & $\\dots$ \\\\\n",
       "\t22 & 0.0664 & 0.0575 & 0.0842 & 0.0372 & 0.0458 & 0.0771 & 0.0771 & 0.113 & 0.2353 & $\\dots$ \\\\\n",
       "\t23 & 0.0099 & 0.0484 & 0.0299 & 0.0297 & 0.0652 & 0.1077 & 0.2363 & 0.2385 & 0.0075 & $\\dots$ \\\\\n",
       "\t24 & 0.0115 & 0.015 & 0.0136 & 0.0076 & 0.0211 & 0.1058 & 0.1023 & 0.044 & 0.0931 & $\\dots$ \\\\\n",
       "\t25 & 0.0293 & 0.0644 & 0.039 & 0.0173 & 0.0476 & 0.0816 & 0.0993 & 0.0315 & 0.0736 & $\\dots$ \\\\\n",
       "\t26 & 0.0201 & 0.0026 & 0.0138 & 0.0062 & 0.0133 & 0.0151 & 0.0541 & 0.021 & 0.0505 & $\\dots$ \\\\\n",
       "\t27 & 0.0151 & 0.032 & 0.0599 & 0.105 & 0.1163 & 0.1734 & 0.1679 & 0.1119 & 0.0889 & $\\dots$ \\\\\n",
       "\t28 & 0.0177 & 0.03 & 0.0288 & 0.0394 & 0.063 & 0.0526 & 0.0688 & 0.0633 & 0.0624 & $\\dots$ \\\\\n",
       "\t29 & 0.01 & 0.0275 & 0.019 & 0.0371 & 0.0416 & 0.0201 & 0.0314 & 0.0651 & 0.1896 & $\\dots$ \\\\\n",
       "\t30 & 0.0189 & 0.0308 & 0.0197 & 0.0622 & 0.008 & 0.0789 & 0.144 & 0.1451 & 0.1789 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m208×61 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m Column2 \u001b[0m\u001b[1m Column3 \u001b[0m\u001b[1m Column4 \u001b[0m\u001b[1m Column5 \u001b[0m\u001b[1m Column6 \u001b[0m\u001b[1m Column7 \u001b[0m\u001b[1m Column8 \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.02     0.0371   0.0428   0.0207   0.0954   0.0986   0.1539   0.1601  ⋯\n",
       "   2 │  0.0453   0.0523   0.0843   0.0689   0.1183   0.2583   0.2156   0.3481\n",
       "   3 │  0.0262   0.0582   0.1099   0.1083   0.0974   0.228    0.2431   0.3771\n",
       "   4 │  0.01     0.0171   0.0623   0.0205   0.0205   0.0368   0.1098   0.1276\n",
       "   5 │  0.0762   0.0666   0.0481   0.0394   0.059    0.0649   0.1209   0.2467  ⋯\n",
       "   6 │  0.0286   0.0453   0.0277   0.0174   0.0384   0.099    0.1201   0.1833\n",
       "   7 │  0.0317   0.0956   0.1321   0.1408   0.1674   0.171    0.0731   0.1401\n",
       "   8 │  0.0519   0.0548   0.0842   0.0319   0.1158   0.0922   0.1027   0.0613\n",
       "   9 │  0.0223   0.0375   0.0484   0.0475   0.0647   0.0591   0.0753   0.0098  ⋯\n",
       "  10 │  0.0164   0.0173   0.0347   0.007    0.0187   0.0671   0.1056   0.0697\n",
       "  11 │  0.0039   0.0063   0.0152   0.0336   0.031    0.0284   0.0396   0.0272\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       " 199 │  0.0238   0.0318   0.0422   0.0399   0.0788   0.0766   0.0881   0.1143\n",
       " 200 │  0.0116   0.0744   0.0367   0.0225   0.0076   0.0545   0.111    0.1069  ⋯\n",
       " 201 │  0.0131   0.0387   0.0329   0.0078   0.0721   0.1341   0.1626   0.1902\n",
       " 202 │  0.0335   0.0258   0.0398   0.057    0.0529   0.1091   0.1709   0.1684\n",
       " 203 │  0.0272   0.0378   0.0488   0.0848   0.1127   0.1103   0.1349   0.2337\n",
       " 204 │  0.0187   0.0346   0.0168   0.0177   0.0393   0.163    0.2028   0.1694  ⋯\n",
       " 205 │  0.0323   0.0101   0.0298   0.0564   0.076    0.0958   0.099    0.1018\n",
       " 206 │  0.0522   0.0437   0.018    0.0292   0.0351   0.1171   0.1257   0.1178\n",
       " 207 │  0.0303   0.0353   0.049    0.0608   0.0167   0.1354   0.1465   0.1123\n",
       " 208 │  0.026    0.0363   0.0136   0.0272   0.0214   0.0338   0.0655   0.14    ⋯\n",
       "\u001b[36m                                                 53 columns and 187 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "MineorRock= DataFrame(CSV.File(\"sonar.csv\",header=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e8bf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208×60 Matrix{Float64}:\n",
       " 0.02    0.0371  0.0428  0.0207  0.0954  …  0.018   0.0084  0.009   0.0032\n",
       " 0.0453  0.0523  0.0843  0.0689  0.1183     0.014   0.0049  0.0052  0.0044\n",
       " 0.0262  0.0582  0.1099  0.1083  0.0974     0.0316  0.0164  0.0095  0.0078\n",
       " 0.01    0.0171  0.0623  0.0205  0.0205     0.005   0.0044  0.004   0.0117\n",
       " 0.0762  0.0666  0.0481  0.0394  0.059      0.0072  0.0048  0.0107  0.0094\n",
       " 0.0286  0.0453  0.0277  0.0174  0.0384  …  0.0057  0.0027  0.0051  0.0062\n",
       " 0.0317  0.0956  0.1321  0.1408  0.1674     0.0092  0.0143  0.0036  0.0103\n",
       " 0.0519  0.0548  0.0842  0.0319  0.1158     0.0085  0.0047  0.0048  0.0053\n",
       " 0.0223  0.0375  0.0484  0.0475  0.0647     0.0065  0.0093  0.0059  0.0022\n",
       " 0.0164  0.0173  0.0347  0.007   0.0187     0.0032  0.0035  0.0056  0.004\n",
       " 0.0039  0.0063  0.0152  0.0336  0.031   …  0.0042  0.0003  0.0053  0.0036\n",
       " 0.0123  0.0309  0.0169  0.0313  0.0358     0.0026  0.0092  0.0009  0.0044\n",
       " 0.0079  0.0086  0.0055  0.025   0.0344     0.0059  0.0058  0.0059  0.0032\n",
       " ⋮                                       ⋱                          \n",
       " 0.005   0.0017  0.027   0.045   0.0958     0.0024  0.0063  0.0017  0.0028\n",
       " 0.0366  0.0421  0.0504  0.025   0.0596     0.0025  0.0017  0.0027  0.0027\n",
       " 0.0238  0.0318  0.0422  0.0399  0.0788     0.0028  0.0013  0.0035  0.006\n",
       " 0.0116  0.0744  0.0367  0.0225  0.0076     0.0037  0.0044  0.0057  0.0035\n",
       " 0.0131  0.0387  0.0329  0.0078  0.0721  …  0.004   0.0009  0.0015  0.0085\n",
       " 0.0335  0.0258  0.0398  0.057   0.0529     0.0045  0.0022  0.0005  0.0031\n",
       " 0.0272  0.0378  0.0488  0.0848  0.1127     0.0054  0.0051  0.0065  0.0103\n",
       " 0.0187  0.0346  0.0168  0.0177  0.0393     0.0065  0.0115  0.0193  0.0157\n",
       " 0.0323  0.0101  0.0298  0.0564  0.076      0.0034  0.0032  0.0062  0.0067\n",
       " 0.0522  0.0437  0.018   0.0292  0.0351  …  0.014   0.0138  0.0077  0.0031\n",
       " 0.0303  0.0353  0.049   0.0608  0.0167     0.0034  0.0079  0.0036  0.0048\n",
       " 0.026   0.0363  0.0136  0.0272  0.0214     0.004   0.0036  0.0061  0.0115"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=Array(MineorRock[:,1:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9affee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208-element Vector{String1}:\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=Array(MineorRock[:,61])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f97e0",
   "metadata": {},
   "source": [
    "Ahora, partimos nuestro Dataset en dos conjuntos, uno para training y otro para testing, como se ve en lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ce259b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\u001b[1m155×61 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m Column2 \u001b[0m\u001b[1m Column3 \u001b[0m\u001b[1m Column4 \u001b[0m\u001b[1m Column5 \u001b[0m\u001b[1m Column6 \u001b[0m\u001b[1m Column7 \u001b[0m\u001b[1m Column8 \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.02     0.0371   0.0428   0.0207   0.0954   0.0986   0.1539   0.1601  ⋯\n",
       "   2 │  0.0262   0.0582   0.1099   0.1083   0.0974   0.228    0.2431   0.3771\n",
       "   3 │  0.01     0.0171   0.0623   0.0205   0.0205   0.0368   0.1098   0.1276\n",
       "   4 │  0.0762   0.0666   0.0481   0.0394   0.059    0.0649   0.1209   0.2467\n",
       "   5 │  0.0317   0.0956   0.1321   0.1408   0.1674   0.171    0.0731   0.1401  ⋯\n",
       "   6 │  0.0519   0.0548   0.0842   0.0319   0.1158   0.0922   0.1027   0.0613\n",
       "   7 │  0.0223   0.0375   0.0484   0.0475   0.0647   0.0591   0.0753   0.0098\n",
       "   8 │  0.0164   0.0173   0.0347   0.007    0.0187   0.0671   0.1056   0.0697\n",
       "   9 │  0.0039   0.0063   0.0152   0.0336   0.031    0.0284   0.0396   0.0272  ⋯\n",
       "  10 │  0.0123   0.0309   0.0169   0.0313   0.0358   0.0102   0.0182   0.0579\n",
       "  11 │  0.0079   0.0086   0.0055   0.025    0.0344   0.0546   0.0528   0.0958\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       " 146 │  0.0366   0.0421   0.0504   0.025    0.0596   0.0252   0.0958   0.0991\n",
       " 147 │  0.0238   0.0318   0.0422   0.0399   0.0788   0.0766   0.0881   0.1143  ⋯\n",
       " 148 │  0.0116   0.0744   0.0367   0.0225   0.0076   0.0545   0.111    0.1069\n",
       " 149 │  0.0335   0.0258   0.0398   0.057    0.0529   0.1091   0.1709   0.1684\n",
       " 150 │  0.0272   0.0378   0.0488   0.0848   0.1127   0.1103   0.1349   0.2337\n",
       " 151 │  0.0187   0.0346   0.0168   0.0177   0.0393   0.163    0.2028   0.1694  ⋯\n",
       " 152 │  0.0323   0.0101   0.0298   0.0564   0.076    0.0958   0.099    0.1018\n",
       " 153 │  0.0522   0.0437   0.018    0.0292   0.0351   0.1171   0.1257   0.1178\n",
       " 154 │  0.0303   0.0353   0.049    0.0608   0.0167   0.1354   0.1465   0.1123\n",
       " 155 │  0.026    0.0363   0.0136   0.0272   0.0214   0.0338   0.0655   0.14    ⋯\n",
       "\u001b[36m                                                 53 columns and 134 rows omitted\u001b[0m, \u001b[1m53×61 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m Column2 \u001b[0m\u001b[1m Column3 \u001b[0m\u001b[1m Column4 \u001b[0m\u001b[1m Column5 \u001b[0m\u001b[1m Column6 \u001b[0m\u001b[1m Column7 \u001b[0m\u001b[1m Column8 \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.0453   0.0523   0.0843   0.0689   0.1183   0.2583   0.2156   0.3481  ⋯\n",
       "   2 │  0.0286   0.0453   0.0277   0.0174   0.0384   0.099    0.1201   0.1833\n",
       "   3 │  0.0124   0.0433   0.0604   0.0449   0.0597   0.0355   0.0531   0.0343\n",
       "   4 │  0.0352   0.0116   0.0191   0.0469   0.0737   0.1185   0.1683   0.1541\n",
       "   5 │  0.027    0.0092   0.0145   0.0278   0.0412   0.0757   0.1026   0.1138  ⋯\n",
       "   6 │  0.0473   0.0509   0.0819   0.1252   0.1783   0.307    0.3008   0.2362\n",
       "   7 │  0.024    0.0218   0.0324   0.0569   0.033    0.0513   0.0897   0.0713\n",
       "   8 │  0.0084   0.0153   0.0291   0.0432   0.0951   0.0752   0.0414   0.0259\n",
       "   9 │  0.0123   0.0022   0.0196   0.0206   0.018    0.0492   0.0033   0.0398  ⋯\n",
       "  10 │  0.0211   0.0319   0.0415   0.0286   0.0121   0.0438   0.1299   0.139\n",
       "  11 │  0.0408   0.0653   0.0397   0.0604   0.0496   0.1817   0.1178   0.1024\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       "  44 │  0.0294   0.0123   0.0117   0.0113   0.0497   0.0998   0.1326   0.1117\n",
       "  45 │  0.0197   0.0394   0.0384   0.0076   0.0251   0.0629   0.0747   0.0578  ⋯\n",
       "  46 │  0.031    0.0221   0.0433   0.0191   0.0964   0.1827   0.1106   0.1702\n",
       "  47 │  0.0096   0.0404   0.0682   0.0688   0.0887   0.0932   0.0955   0.214\n",
       "  48 │  0.0269   0.0383   0.0505   0.0707   0.1313   0.2103   0.2263   0.2524\n",
       "  49 │  0.034    0.0625   0.0381   0.0257   0.0441   0.1027   0.1287   0.185   ⋯\n",
       "  50 │  0.0203   0.0121   0.038    0.0128   0.0537   0.0874   0.1021   0.0852\n",
       "  51 │  0.0392   0.0108   0.0267   0.0257   0.041    0.0491   0.1053   0.169\n",
       "  52 │  0.0129   0.0141   0.0309   0.0375   0.0767   0.0787   0.0662   0.1108\n",
       "  53 │  0.0131   0.0387   0.0329   0.0078   0.0721   0.1341   0.1626   0.1902  ⋯\n",
       "\u001b[36m                                                  53 columns and 32 rows omitted\u001b[0m)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Lathe.preprocess: TrainTestSplit\n",
    "train, test = TrainTestSplit(MineorRock,.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd7e88",
   "metadata": {},
   "source": [
    "Dividimos en las etiquetas y los vectores de caracteristicas los anteriores subconjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de3bc1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53-element Vector{String1}:\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1=Array(train[:,1:60])\n",
    "x_2=Array(test[:,1:60])\n",
    "y_1=Array(train[:,61])\n",
    "y_2=Array(test[:,61])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8ce1b",
   "metadata": {},
   "source": [
    "Importamos la Librerias \"GLM\" y \"ScikitLearn\", las cuales tienen ya implementados los algoritmos que hemos visto hasta el momento en clase. \n",
    "\n",
    "A continuacion, en cada uno de los distintos algoritmos, ponemos a trabajar la maquina en nuestro conjunto de datos de training, y luego que la maquina aprenda, usamos el conjunto de testing para predecir que tan bien se logro el objetivo. Para esto, calculamos una metrica llamada \"accuracy\", la cual se define como el número de predicciones correctas dividido por el número total de predicciones, multiplicado por 100 . Usamos esta metrica pues es una de las faciles de implementar.\n",
    "\n",
    "A continuacion estan cada uno de los algoritmos expuestos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60939e",
   "metadata": {},
   "source": [
    "# Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09590564",
   "metadata": {},
   "outputs": [],
   "source": [
    "using GLM\n",
    "using ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb257d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant LogisticRegression. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "@sk_import linear_model: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee8471b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR= LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa43f14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LogisticRegression()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn: fit!\n",
    "fit!(LR,x_1,y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e5ebb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53-element Vector{Any}:\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn: predict\n",
    "PreLR=predict(LR,x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36c9b8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <function accuracy_score at 0x00000000A9314820>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import metrics: accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cdd5c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8301886792452831"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acLR=accuracy_score(PreLR,y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3121e",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd5bb222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(LIBSVM.Kernel.RadialBasis, 0.016666666666666666, nothing, 1.0, 3, 0.0, 0.001, true, false, false, LIBSVM.SVM{String1, LIBSVM.Kernel.KERNEL}(SVC, LIBSVM.Kernel.RadialBasis, nothing, 60, 155, 2, String1[\"R\", \"M\"], Int32[1, 2], Float64[], Int32[], LIBSVM.SupportVectors{Vector{String1}, Matrix{Float64}}(143, Int32[71, 72], String1[\"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"  …  \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\"], [0.02 0.0262 … 0.0303 0.026; 0.0371 0.0582 … 0.0353 0.0363; … ; 0.009 0.0095 … 0.0036 0.0061; 0.0032 0.0078 … 0.0048 0.0115], Int32[1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  144, 146, 147, 148, 150, 151, 152, 153, 154, 155], LIBSVM.SVMNode[LIBSVM.SVMNode(1, 0.02), LIBSVM.SVMNode(1, 0.0262), LIBSVM.SVMNode(1, 0.01), LIBSVM.SVMNode(1, 0.0762), LIBSVM.SVMNode(1, 0.0317), LIBSVM.SVMNode(1, 0.0519), LIBSVM.SVMNode(1, 0.0223), LIBSVM.SVMNode(1, 0.0164), LIBSVM.SVMNode(1, 0.0039), LIBSVM.SVMNode(1, 0.0123)  …  LIBSVM.SVMNode(1, 0.0056), LIBSVM.SVMNode(1, 0.0366), LIBSVM.SVMNode(1, 0.0238), LIBSVM.SVMNode(1, 0.0116), LIBSVM.SVMNode(1, 0.0272), LIBSVM.SVMNode(1, 0.0187), LIBSVM.SVMNode(1, 0.0323), LIBSVM.SVMNode(1, 0.0522), LIBSVM.SVMNode(1, 0.0303), LIBSVM.SVMNode(1, 0.026)]), 0.0, [1.0; 1.0; … ; -1.0; -1.0;;], Float64[], Float64[], [0.34000277519226074], 3, 0.016666666666666666, 200.0, 0.001, 1.0, 0.5, 0.1, true, false))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LIBSVM\n",
    "SVMa= fit!(SVC(), x_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f768af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53-element Vector{String1}:\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PreSVMa=predict(SVMa,x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "057723b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5283018867924528"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acSVM=accuracy_score(PreSVMa,y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6358d1",
   "metadata": {},
   "source": [
    "# Árboles de Decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac28c1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.tree._classes.DecisionTreeClassifier'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import tree: DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6b9cdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject DecisionTreeClassifier()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_model=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b75fbe73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject DecisionTreeClassifier()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(tree_model,x_1,y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c258bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53-element Vector{Any}:\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredTree=predict(tree_model,x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6ccc44ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7924528301886793"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acTree=accuracy_score(PredTree,y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92003b",
   "metadata": {},
   "source": [
    "# Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbfce8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._base.LinearRegression'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import linear_model: LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fbc8dd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LinearRegression()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Linear=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "216bf739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155-element Vector{Any}:\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " ⋮\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_11=replace(y_1,\"R\" => 0)\n",
    "y_12=replace(y_11,\"M\" => 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48b2d7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject LinearRegression()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Linear,x_1,y_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8a8034ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53-element Vector{Float64}:\n",
       " -0.46385644552789324\n",
       "  0.5380814975027547\n",
       "  0.13965462582330185\n",
       "  0.15010534632373826\n",
       " -0.3940556094903075\n",
       "  0.7165463794116752\n",
       " -0.2959733812036511\n",
       "  0.10119366887049666\n",
       "  0.2719771684123119\n",
       " -1.0677707894715525\n",
       "  0.6046321346080232\n",
       "  0.30490650922562246\n",
       "  0.04423783223340472\n",
       "  ⋮\n",
       "  0.613047452346699\n",
       "  0.866914680032113\n",
       "  1.0226806900807488\n",
       "  0.27244071550559096\n",
       "  1.490260107918587\n",
       "  0.9148361177242146\n",
       "  1.4927103951366796\n",
       "  1.2907313654517445\n",
       "  0.22348686443898064\n",
       "  0.4045715975959247\n",
       "  0.9675973689177296\n",
       "  1.0805058259652953"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredLinear=predict(Linear,x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "67e507a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "for i in 1:53\n",
    "    if PredLinear[i]<0.5\n",
    "        PredLinear[i]=0\n",
    "    else\n",
    "        PredLinear[i]=1\n",
    "    end\n",
    "end\n",
    "println(PredLinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0accec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53-element Vector{Any}:\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " ⋮\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"M\"\n",
       " \"R\"\n",
       " \"R\"\n",
       " \"M\"\n",
       " \"M\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredLinear2=replace(PredLinear,0.0 => \"R\")\n",
    "PredLinear3=replace(PredLinear2, 1.0 => \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2d6896cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8301886792452831"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acLinear=accuracy_score(PredLinear3,y_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d1209",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1565633b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8301886792452831"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max=maximum([acLR,acTree,acSVM,acLinear])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "46cce264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mejor algoritmo es el de Regresion Lineal con un accuracy de 0.8301886792452831"
     ]
    }
   ],
   "source": [
    "if max==acLR\n",
    "    print(\"El mejor algoritmo es el de Regresion Lineal con un accuracy de $acLR\")\n",
    "elseif max==acTree\n",
    "     print(\"El mejor algoritmo es el de Arboles de Decision con un accuracy de $acTree\")\n",
    "elseif max==acSVM\n",
    "     print(\"El mejor algoritmo es el de SVM con un accuracy de $acSVM\")\n",
    "else max==acLinear\n",
    "     print(\"El mejor algoritmo es el de Regresion Lineal con un accuracy $acLinear\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
